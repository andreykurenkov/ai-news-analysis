---
title: "Skynet This Week #8: top AI news from 08/27/18 - 09/10/18"
image:
  feature: assets/img/digests/8/main.jpg
  credit: Google Blog "Introducing the Inclusive Images Competition" Post
excerpt: "Safe deployment, standards talk, government policy, and more!"
categories: [digests]
permalink: /digests/the-eighth
---

Our bi-weekly quick take on a bunch of the most important recent media stories about AI for the period 08/27/18 - 09/10/18.

## Advances & Business

####  [Safety-first AI for autonomous data centre cooling and industrial control](https://deepmind.com/blog/safety-first-ai-autonomous-data-centre-cooling-and-industrial-control/)
**Amanda Gasparik, Chris Gamble, Jim Gao, Deepmind**

>  Rules don‚Äôt get better over time, but AI does.

In [2016](https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/), Deepmind developed a machine learning based recommendation system to reduce Google‚Äôs data center cooling bills by upto 40%. Based on feedback from operators, Deepmind has now developed a system that autonomously controls data center cooling with strict safety constraints in place. This new system delivers savings of 30% on average with further improvement expected over time. 

####  [OpenAI‚Äôs Dota 2 defeat is still a win for artificial intelligence](https://www.theverge.com/2018/8/28/17787610/openai-dota-2-bots-ai-lost-international-reinforcement-learning)
**James Vincent, The Verge**

The OpenAI Five, an AI system designed by OpenAI to play Dota 5v5, recently lost in a tightly fought best-of-three contest against a team of 99th percentile Dota players where the humans won two out of three matches. In spite of the loss, humans are now learning from the OpenAI Five game play. Key machine learning experts also believe that the bots, which can go through 100 lifetimes of human Dota experience per day, have a lot more potential both in terms of research and engineering improvements. 

####  [ANZ bank unpicking neural networks in effort to avoid dangers of deep learning](https://www.zdnet.com/article/anz-bank-unpicking-neural-networks-in-effort-to-avoid-dangers-of-deep-learning/)
**Chris Duckett, ZD Net**

Deep Learning has been being applied in all sorts of areas, but with the rising popularity of such tools there also comes the need to be aware of their limitations. If not careful, an AI algorithm may be [unintentionally made to be biased](https://www.skynettoday.com/briefs/aclu-amazon-rekognition) and misused. So it is good to read that ANZ bank is not rushing such their shiny new neural net based risk assessment model into production, and is instead working to understand it better first. The Head of Retail Risk at ANZ Jason Humphrey nicely summed up the things to keep in mind when using neural networks:

> "In a deep-learning environment, it becomes very difficult to work out the factors that were the most predictive for this instance, or for this customer," he said. "Before we roll out any deep-learning models, we need to solve for that -- even though it's not legislated here. I think it is good practice to be able to know why decisions are being made."
> ‚Ä¶
> "The biggest danger in terms of deep learning is because it is bringing in new attributes and new correlations we've never seen, is that the things that traditionally we have never seen that could be creating bias, that we wouldn't know to look for to say, 'that's something we shouldn't do'," Humphrey explained.

#### [These Entrepreneurs Are Taking on Bias in Artificial Intelligence](https://www.entrepreneur.com/article/319228)
**Liz Webber, Entrepreneur.com**

As AI software is increasingly being used for decision-making in everyday lives, a growing number of AI scientists and AI entrepreneurs are stepping up to the challenge of biased decision-making due to biased datasets. Proposed solutions range from challenging standard practices in machine learning to diversifying the talent pool for AI researchers and engineers. Incidentally, bias in machine learning algorithms is often a reflection of the socioeconomic homogeneity of the developers who built them. 

<hr>


## Concerns & Hype

####  [I Used AI To Clone My Voice And Trick My Mom Into Thinking It Was Me](https://www.buzzfeednews.com/article/charliewarzel/i-used-ai-to-clone-my-voice-and-trick-my-mom-into-thinking)
**Charlie Warzel, Buzzfeed News**

As [we have covered before](https://www.skynettoday.com/briefs/deepfakes/), the ability of AI algorithms to produce audio and video that convincingly mimics people is a topic of significant concern. Buzzfeed‚Äôs Charlie Warzel has documented how close we are to the wide spread usability of such algorithms being a reality - he could already use a commercial product to have a ‚Äòconversation‚Äô with his mom using audio entirely generated by the algorithm. We may soon all have to become used to being a little more skeptical of the voices we hear over the phone soon...

> ‚ÄúNot only did my Lyrebird voice fool my mom, she had a hard time believing it was created by AI. "I didn't know something like that was possible," she told me later on, reminding me that audio and video manipulation to her sounds more like science fiction than reality. "I never doubted for a second that it was you."‚Äù

####  [Factsheets for AI Services](https://www.ibm.com/blogs/research/2018/08/factsheets-ai/)
**Aleksandra Mojsilovic, IBM Research**

> Concerns about safety, transparency, and bias in AI are widespread, and it is easy to see how they erode trust in these systems. Part of the problem is a lack of standard practices to document how an AI service was created, tested, trained, deployed, and evaluated; how it should operate; and how it should (and should not) be used. To address this need, my colleagues and I recently proposed the concept of factsheets for AI services.

IBM Research have proposed using factsheets for AI services to increase transparency and engender trust in them. These factsheets, which would be voluntarily released by AI service providers, aim to effectively measure and communicate performance with respect to fairness, safety, reliability, explainability and accountability. 

####  [U.S. Senator Bans Funding for Beerbots That Don‚Äôt Exist](https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/senator-bans-funding-for-beerbots-that-dont-exist)
**Evan Ackerman, IEEE Spectrum**

In 2015, a team from MIT developed algorithms for coordination between multiple robots under uncertainty. The students set up a beer delivery task to demonstrate the effectiveness of their algorithms. US Senator Jeff Flake, has taken this demo out of context and used it as an example of wasteful spending by the Department of Defense. 

<figure>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Senator <a href="https://twitter.com/JeffFlake?ref_src=twsrc%5Etfw">@jeffflake</a> presents a bent view of manipulation research as building a <a href="https://twitter.com/hashtag/beerbot?src=hash&amp;ref_src=twsrc%5Etfw">#beerbot</a> and prohibits use of federal grants for robot bartenders. He missed the part where they said &quot;This is a demo&quot;ü§¶üèª‚Äç‚ôÇÔ∏è <a href="https://t.co/sIsHT8A1fb">https://t.co/sIsHT8A1fb</a></p>&mdash; Animesh Garg (@animesh_garg) <a href="https://twitter.com/animesh_garg/status/1034893526192902144?ref_src=twsrc%5Etfw">August 29, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
</figure>

<hr>

## Analysis & Policy

####  [Defense Department pledges billions toward artificial intelligence research](https://www.washingtonpost.com/technology/2018/09/07/defense-department-pledges-billions-toward-artificial-intelligence-research/)
**Drew Harwell, Washington Post**

DARPA has announced plans to invest $2 Billion towards various programs aiming to advance artificial intelligence. These programs will be in addition to the existing 20 plus programs dedicated to AI, and will focus on logistical, ethical, safety, privacy problems and explainable AI. This move will put the US at odds with Silicon Valley companies and academics who have been vocal about not developing AI for military use. 

> ‚ÄúThis is a massive deal. It‚Äôs the first indication that the United States is addressing advanced AI technology with the scale and funding and seriousness that the issue demands,‚Äù said Gregory C. Allen, an adjunct fellow specializing in AI and robotics for the Center for a New American Security, a Washington think tank. ‚ÄúWe‚Äôve seen China willing to devote billions to this issue, and this is the first time the U.S. has done the same.‚Äù

####  [Inside the United Nations‚Äô effort to regulate autonomous killer robots](https://www.theverge.com/2018/8/27/17786080/united-nations-un-autonomous-killer-robots-regulation-conference)
**Sono Motoyoma, The Verge**

The Verge‚Äôs Sono Motoyoma sat down with the chair of the United Nations‚Äô Convention on Conventional Weapons, Amandeep Gill to talk about lethal autonomous weapons. Gill talks about how policy should be tech-neutral, different questions that he hopes will be addressed at the convention, and how AI weapons development differs from previously regulated weapons like Nuclear bombs. 

> ‚ÄúI think making policy out of fear is not a good idea. So as serious policy practitioners, we have to look at what has become of the situation in terms of technology development and what is likely to happen.‚Äù

####  [China‚Äôs AI push raises fears over widespread job cuts](https://www.ft.com/content/1e2db400-ac2d-11e8-94bd-cba20d67390c)
**YUAN YANG, The Financial Times**

> ‚ÄúAutomation has replaced the jobs of up to 40 per cent of workers in some Chinese industrial companies over the past three years, highlighting the effects of Beijing‚Äôs push to upgrade its technological base and become a world superpower in artificial intelligence.‚Äù

China‚Äôs government is making a strong push to upgrade manufacturing tech, but the increased automation has correspondingly led increased job losses for low skilled laborers whose work is being automated. An interesting development and a good reminder that AI should be understood as the next wave of labor automation.

<hr>

## Expert Opinions & Discussion within the field

####  [Introducing the Inclusive Images Competition](https://ai.googleblog.com/2018/09/introducing-inclusive-images-competition.html)
**Tulsee Doshi, Google AI Blog**

Google has announced the Inclusive Images Competition on Kaggle. The competition aims to help further development of ML methods that are robust and inclusive even when learning from imperfect data sources. The competition challenge involves using Open Images, an image classification dataset majorly sampled from North America and Europe. The models trained on this dataset will be evaluated on crowdsourced images from different geographic regions across the globe. The deadline for the challenge is November 5th. 

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">We launched an &#39;Inclusive Images Challenge&#39; to promote inclusive development of image recognition models across geographically diverse datasets.<br><br>Results will be presented at <a href="https://twitter.com/hashtag/NIPS2018?src=hash&amp;ref_src=twsrc%5Etfw">#NIPS2018</a>.<br>More info in the Google AI blog: <a href="https://t.co/oWezKUa0NR">https://t.co/oWezKUa0NR</a> <a href="https://t.co/DAsgd0YntI">https://t.co/DAsgd0YntI</a></p>&mdash; Jeff Dean (@JeffDean) <a href="https://twitter.com/JeffDean/status/1037753012670750720?ref_src=twsrc%5Etfw">September 6, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 

####  [Should Evil AI Research Be Published? Five Experts Weigh In](https://futurism.com/should-evil-ai-research-be-published-five-experts-weigh-in/)
**Dan Robitzski, Futurism**

While ‚ÄúShould Evil AI research be published?‚Äù is a rhetorical question, it provides for an interesting thought experiment. Five AI thought experts weighed in with their thoughts. Their answers ranged from it should be published, to thinking about responsible use and impact before publishing. 

<hr>

## Explainers

####  [Why Love Generative Art?](https://www.artnome.com/news/2018/8/8/why-love-generative-art)
**Jason Bailey, Artnome.com**

With AI generated art gaining in popularity, this well-written post about the history of Generative art is a compelling read. It covers the origins of the field and looks at the various artists who have influenced generative art in general. It also talks about how generative art is like conventional art and requires significant human contribution. 

####  [Face detection - An overview and comparison of different solutions](https://www.liip.ch/en/blog/face-detection-an-overview-and-comparison-of-different-solutions-part1)
**David Pacassi Torrico, liip.ch**

This article looks at the methods, pricing and results of commercially available face detection systems from Amazon, Google, IBM and Microsoft. 

#### [Aurora‚Äôs Approach to Development of Self Driving Cars](https://medium.com/aurora-blog/auroras-approach-to-development-5e42fec2ee4b)
**The Aurora Team, Medium**

Aurora, a self-driving car startup share their thinking and approach to self-driving technology. They talk about self-driving as an applied science problem, testing, integrating machine learning and their process for building these systems. 


## Awesome Videos

####  [AI co-produced Taryn Southern's new album](https://youtu.be/hVq5ZcE2d_Q)
**Dani Deahl, The Verge**

<iframe width="560" height="315" src="https://www.youtube.com/embed/hVq5ZcE2d_Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

####  [How AI can save our humanity](https://youtu.be/ajGgd9Ld-Wc)
**Kai-Fu Lee, TED**

<iframe width="560" height="315" src="https://www.youtube.com/embed/ajGgd9Ld-Wc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>


## Favourite Tweet
<figure>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Me: Wait... <a href="https://twitter.com/elonmusk?ref_src=twsrc%5Etfw">@elonmusk</a> a global influencer in <a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a> and <a href="https://twitter.com/hashtag/MachineLearning?src=hash&amp;ref_src=twsrc%5Etfw">#MachineLearning</a>?<br><br>Me: Oh...<br><br>Me: Really????<br><br>Me: Since when?<br><br>Me: Terminator pics, misleading headlines, <a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a> dystopia, <a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a> hype, and now this... :-/ <a href="https://t.co/kuRzj8aYXF">https://t.co/kuRzj8aYXF</a></p>&mdash; Dagmar Monett (@dmonett) <a href="https://twitter.com/dmonett/status/1038324510439731200?ref_src=twsrc%5Etfw">September 8, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</figure>

## Favorite meme
<figure>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">RL agorithms&#39; performance on Atari, ranked: <br><br>16. Evaluation <br>15. Conditions<br>14. Vary<br>13. Across<br>12. Papers<br>11. There&#39;s<br>10. No<br>9. Single<br>8. Metric<br>7. Picking<br>6. A<br>5. Single<br>4. Algorithm<br>3. Is<br>2. Misleading<br>1. Ape-X DQN</p>&mdash; Miles Brundage (@Miles_Brundage) <a href="https://twitter.com/Miles_Brundage/status/1035266957631778816?ref_src=twsrc%5Etfw">August 30, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</figure>


<hr>

That's all for this digest! If you are not subscribed and liked this, feel free to subscribe below!









