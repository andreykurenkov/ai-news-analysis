---
title: "Last Week in AI #36"
excerpt: "Interactive fairness games, manipulating cubes, restoring ancient texts and more!"
image: 
  feature: assets/img/digests/36/main.png
  credit: <a href="https://www.technologyreview.com/s/613508/ai-fairer-than-judge-criminal-risk-assessment-algorithm/"> Selman Design / MIT Technology Review </a>
categories: [digests]
permalink: /digests/the-thirty-sixth
---

### Mini Briefs

#### [Can you make AI fairer than a judge? Play our courtroom algorithm game](https://www.technologyreview.com/s/613508/ai-fairer-than-judge-criminal-risk-assessment-algorithm/) 

Algorithmic fairness came under the spotlight in 2016, when ProPublic published a [report](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) showing that COMPAS, an algorithm used to assess recidivism risk for criminals, was twice as biased for blacks compared to whites. This article examines algorithmic fairness and definitions of fairness by walking the reader through an interactive game to improve COMPAS.  

> Predictions reflect the data used to make them—whether by algorithm or not. If black defendants are arrested at a higher rate than white defendants in the real world, they will have a higher rate of predicted arrest as well. This means they will also have higher risk scores on average, and a larger percentage of them will be labeled high-risk—both correctly and incorrectly. This is true no matter what algorithm is used, as long as it’s designed so that each risk score means the same thing regardless of race.

The article uses the interactive game to show you that there are multiple possible (and defensible) definitions of fairness, and that satisfying multiple definitions at the same time is impossible. Arriving at the right definition of fairness for a given problem is an open debate which cannot be solved. Even human judges are forced to make trade-offs between different definitions of fairness. However, humans judges and algorithmic decision-making systems differ in terms of accountability. Currently, algorithmic decision-making systems like COMPAS are "trade-secrets" that cannot be publicly scrutinized and thus, held accountable.  

> So what should regulators do? The proposed Algorithmic Accountability Act of 2019 is an example of a good start, says Andrew Selbst, a law professor at the University of California who specializes in AI and the law. The bill, which seeks to regulate bias in automated decision-making systems, has two notable features that serve as a template for future legislation.  

The game highlights the difficulties in making decisions with respect to a population and the conflicts and tensions in "fairness". There is now, regulatory acknowledgement of the problem and encouraging signs that policy will follow. However, there are tough questions to be asked, and caution to be used when dealing with these systems.  

### Advances & Business

* [Researchers design new material using artificial intelligence](https://phys.org/news/2019-10-material-artificial-intelligence.html) - Metamaterial created with Artificial Intelligence that transforms a brittle material into a sponge-like material. Unlike a sponge, this metamaterial is stiff until a critical force is reached after which it becomes easily compressible.

* [Solving Rubik’s Cube with a Robot Hand](https://openai.com/blog/solving-rubiks-cube/) - OpanAI trained a pair of neural networks to solve the Rubik’s Cube with a human-like robot hand. The neural networks are trained entirely in simulation, using the same reinforcement learning code as OpenAI Five paired with a new technique called Automatic Domain Randomization (ADR).

* [Restoring ancient text using deep learning: a case study on Greek epigraphy](https://deepmind.com/research/publications/Restoring-ancient-text-using-deep-learning-a-case-study-on-Greek-epigraphy) - This work presents PYTHIA, the first ancient text restoration model that recovers missing characters from a damaged text input using deep neural networks.

* [Data-labelling startups want to help improve corporate AI](https://www.economist.com/business/2019/10/19/data-labelling-startups-want-to-help-improve-corporate-ai) - A clutch of firms is generating the feedstock for machine-learning algorithms: tagged data.

* [How Artifical Intelligence Is Advancing Precision Medicine](https://www.forbes.com/sites/nicolemartin1/2019/10/18/how-artifical-intelligence-is-advancing-precision-medicine/) - Artificial intelligence and machine learning have been utilized for years in the field of healthcare and continue to grow tremendously each year with its ability to advance medicine and discoveries in the industry.

### Concerns & Hype

* [Why Solving a Rubik's Cube Does Not Signal Robot Supremacy](https://www.wired.com/story/why-solving-rubiks-cube-not-signal-robot-supremacy/) - If someone can solve a Rubik’s Cube, you might safely assume they are both nimble-fingered and good at puzzles. That may not be true for a cube-conquering robot.

* [AI Could Prevent Marital Arguments Before They Even Begin](https://www.wsj.com/articles/ai-could-prevent-marital-arguments-before-they-even-begin-11570807573) - Researchers use listening devices and algorithms to detect speech patterns that typically precede marital fights.

* [You’re in a Police Lineup, Right Now](https://www.nytimes.com/2019/10/15/opinion/facial-recognition-police.html) - In this Video Op-Ed, Clare Garvie demands the United States government hit pause on face recognition. She argues that while this convenient technology may seem benign to those who feel they have nothing to hide, face recognition is something we should all fear.

* [YouTube thinkfluencer Siraj Raval admits he plagiarized boffins' neural qubit papers – as ESA axes his workshop](https://www.theregister.co.uk/2019/10/14/ravel_ai_youtube/) - Internet celeb Siraj Raval’s reputation continues spiraling downward – after he admitted plagiarizing real scientists' work to produce a paper on neural qubits.

### Analysis & Policy

* [No one has a damn clue how many jobs will be lost to automation](https://www.vox.com/2020-presidential-election/2019/10/15/20916567/democratic-debate-warren-yang-automation-trade-robots) - Elizabeth Warren and Andrew Yang clashed over a highly misunderstood issue during the Democratic debate. No one has any clue how many future jobs may be lost to automation; the estimates vary wildly and the methodology for trying to guess is questionable.

* [Adopting AI in Health Care Will Be Slow and Difficult](https://hbr.org/2019/10/adopting-ai-in-health-care-will-be-slow-and-difficult) - Artificial intelligence, including machine learning, presents exciting opportunities to transform the health and life sciences spaces. It offers tantalizing prospects for swifter, more accurate clinical decision making and amplified R&D capabilities. However, open issues around regulation and clinical relevance remain.

* [California Governor Signs Bill Banning Facial Recognition Tech Use By State's Law Enforcement Agencies](https://www.techdirt.com/articles/20191011/18013143178/california-governor-signs-bill-banning-facial-recognition-tech-use-states-law-enforcement-agencies.shtml) - California has become the first state in the US to ban facial recognition tech use by local cops.

### Expert Opinions & Discussion within the field

* [The current state of AI and Deep Learning: A reply to Yoshua Bengio](https://medium.com/@GaryMarcus/the-current-state-of-ai-and-deep-learning-a-reply-to-yoshua-bengio-77952ead7970) - Gary Marcus' reply to Yoshua Bengio about the current state of AI and Deep Learning.

* [The promise and peril of AI](https://www.economist.com/podcasts/2019/10/09/the-promise-and-peril-of-ai) - Artificial Intelligence is on track to become a mainstream technology, on a par with electricity or computing. But in order to flourish it needs to overcome several challenges.

* [More compute is not the solution for AGI](https://buzzrobot.com/more-compute-is-not-the-solution-for-agi-722e3c20132f) - There are different bets on how to achieve AGI (Artificial General Intelligence). It’s nearly impossible to predict which path has the best probability of succeeding. It’s still worth considering the possible approaches, though.

### Explainers

* [Training real AI with fake data](https://www.axios.com/ai-synthetic-data-deep-learning-f97fd71d-6bbb-4bc0-85ac-b16e876f76c2.html) - AI systems have an endless appetite for data. Gathering and labeling data is expensive and time consuming, and in some cases impossible. So companies are teaching AI systems with fake photos and videos, sometimes also generated by AI, that stand in for the real thing.

* [Neural nets are just people all the way down](https://vicki.substack.com/p/neural-nets-are-just-people-all-the) - Down the rabbit hole of how much of Machine learning is really powered by machines?

* [Uncertainty Quantification in Deep Learning](https://www.inovex.de/blog/uncertainty-quantification-deep-learning/) - While it is not usually possible to guarantee that deep learning models to be absolutely perfect, it could be useful to know how certain they are with their predictions. This, requires models to be aware of their prediction accuracy for a given input.

<hr>

That's all for this week! If you are not subscribed and liked this, feel free to subscribe below!
