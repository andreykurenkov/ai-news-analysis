{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import texthero as hero\n",
    "import os\n",
    "import nltk\n",
    "from newspaper import Article\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import scattertext as st\n",
    "import pytextrank\n",
    "import spacy\n",
    "from pprint import pprint\n",
    "import datetime \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_MAPPINGS = {\n",
    "    \"The Hype\": \"Concerns & Hype\",\n",
    "    \"The Panic\": \"Concerns & Hype\",\n",
    "    \"The good coverage\": \"Advances & Business\",\n",
    "    \"Expert Opinions & Discussion within the field\": \"Expert Opinions & Discussion within the field\",\n",
    "    \"Explainers\": \"Explainers\",\n",
    "    \"AI Advances\": \"Advances & Business\",\n",
    "    \"AI Worries\": \"Concerns & Hype\",\n",
    "    \"Advances & Business\": \"Advances & Business\",\n",
    "    \"Concerns & Hype\": \"Concerns & Hype\",\n",
    "    \"Analysis & Policy\": \"Analysis & Policy\",\n",
    "    \"Mini Briefs\": \"Mini Briefs\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(file_name):\n",
    "    with open(file_name,'r') as f:\n",
    "        current_category = None\n",
    "        articles = []\n",
    "        for line in f:\n",
    "            for c in CATEGORY_MAPPINGS.keys():\n",
    "                if c in line:\n",
    "                    current_category = CATEGORY_MAPPINGS[c]\n",
    "            if current_category and '[' in line and '(' in line:\n",
    "                title = line.split('[')[1].split(']')[0]\n",
    "                url = line.split('(')[1].split(')')[0]\n",
    "                if len(title.split(' '))<4:\n",
    "                    continue\n",
    "                if ' - ' in line:\n",
    "                    excerpt = line.split(' - ')[1].strip()\n",
    "                else:\n",
    "                    excerpt = ''\n",
    "                article = Article(url)\n",
    "                try: \n",
    "                    article.download()\n",
    "                    article.parse()\n",
    "                    authors = article.authors\n",
    "                    date = article.publish_date\n",
    "                    text = article.text\n",
    "                    top_image = article.top_image\n",
    "                    article.nlp()\n",
    "                    keywords = article.keywords\n",
    "                    summary = article.summary\n",
    "                except:\n",
    "                    authors=None\n",
    "                    date=None\n",
    "                    keywords=[]\n",
    "                    text=''\n",
    "                    summary=title\n",
    "                articles.append([str(current_category), \n",
    "                                 title, \n",
    "                                 date, \n",
    "                                 url, \n",
    "                                 excerpt, \n",
    "                                 authors, \n",
    "                                 keywords, \n",
    "                                 summary,\n",
    "                                 text])\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edition_date_mapping = {}\n",
    "all_articles = []\n",
    "category_counts = {}\n",
    "for file_name in os.listdir('digests'):\n",
    "    if '.md' not in file_name or 'year' in file_name:\n",
    "        continue\n",
    "    name_parts = file_name.split('.')[0].split('-')\n",
    "    year = int(name_parts[0])\n",
    "    month = int(name_parts[1])\n",
    "    day = int(name_parts[2])\n",
    "    edition = int(name_parts[3])\n",
    "    edition_date = datetime.datetime(year, month, day)\n",
    "    edition_date_mapping[edition] = edition_date\n",
    "    articles = parse_file(os.path.join('digests',file_name))\n",
    "    for article in articles:\n",
    "        article.insert(0,edition_date)\n",
    "        article.insert(0,edition)\n",
    "    all_articles+=articles\n",
    "    for article in articles:\n",
    "        if article[0] not in category_counts:\n",
    "            category_counts[article[2]]=0\n",
    "        category_counts[article[2]]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_articles))\n",
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_articles, columns =['edition', 'digest_date', 'category', 'title', 'article_date', 'url', 'excerpt', 'authors', 'keywords', 'summary', 'text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (article[2], word.lower())\n",
    "    for article in all_articles\n",
    "    for word in nltk.tokenize.word_tokenize(article[-2]))\n",
    "modals = ['ai',\n",
    " 'bias',\n",
    " 'neural',\n",
    " 'robot',\n",
    " 'artificial',\n",
    " 'facial']\n",
    "cfd.tabulate(categories, samples=modals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = []\n",
    "sorted_articles = sorted(all_articles, key = lambda x:x[1])\n",
    "for i,article in enumerate(sorted_articles):\n",
    "    x = article[1]\n",
    "    d = {'Date': x, 'Article Count': i}\n",
    "    dicts.append(d)\n",
    "df_plot = pd.DataFrame(dicts)\n",
    "df_plot.plot(x='Date',y='Article Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (article[0], word.lower())\n",
    "    for article in all_articles\n",
    "    for word in nltk.tokenize.word_tokenize(article[3]))\n",
    "\n",
    "def plot_trends(words, synonyms={}):\n",
    "    dicts = []\n",
    "    word_totals = {word:0 for word in words}\n",
    "    for x in sorted(df['edition'].unique()):\n",
    "        word_counts = cfd[x]\n",
    "        d = {'Date': edition_date_mapping[x]}\n",
    "\n",
    "        for word in words:\n",
    "            word_key = word\n",
    "            if word in synonyms:\n",
    "                word_key = synonyms[word]\n",
    "            word_totals[word_key]+=word_counts[word]\n",
    "            d[word_key] = word_totals[word_key]\n",
    "        dicts.append(d)\n",
    "    df_plot = pd.DataFrame(dicts)\n",
    "    df_plot.plot(x='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trends(['ai', 'robot', 'neural', 'deep', 'researchers', 'learning', 'artificial'], {'learned': 'learning', 'research':'researchers'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trends(['robot', 'neural', 'deep', 'researchers', 'learning', 'artificial'], {'learned': 'learning', 'research':'researchers'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['biased', 'bias', \n",
    "         'surveillance',  'facial', \n",
    "         'coronavirus', 'covid', 'covid-19',\n",
    "         'fake', 'deepfake', 'deepfakes',\n",
    "         'military', 'weapon',\n",
    "         'jobs', 'automation']\n",
    "synonyms = {'covid-19': 'coronavirus',  'covid':'coronavirus', \n",
    "            'deepfakes':'deepfake', 'deepfake': 'fake',\n",
    "            'weapon': 'military',\n",
    "            'bias': 'biased', \n",
    "            'automation': 'jobs'}\n",
    "plot_trends(words, synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['biased', 'bias', \n",
    "         'surveillance', \n",
    "         'coronavirus', 'covid', 'covid-19',\n",
    "         'fake', 'deepfake', 'deepfakes',\n",
    "         'military', 'weapon',\n",
    "         'jobs', 'automation']\n",
    "synonyms = {'covid-19': 'coronavirus',  'covid':'coronavirus', \n",
    "            'deepfakes':'deepfake', 'deepfake': 'fake',\n",
    "            'weapon': 'military',\n",
    "            'bias': 'biased', \n",
    "            'automation': 'jobs'}\n",
    "plot_trends(words, synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['image', 'language', \n",
    "         'robot', 'bot',\n",
    "         'coronavirus', 'covid', 'covid-19',\n",
    "         'medical', 'medicine', 'diagnose',\n",
    "         'predict', 'climate']\n",
    "synonyms = {'covid-19': 'coronavirus',  'covid':'coronavirus', \n",
    "            'diagnose': 'medical', 'medicine': 'medical', 'bot': 'robot'}\n",
    "plot_trends(words, synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['openai', 'deepmind', 'google',  'microsoft', 'amazon', 'facebook', 'stanford', 'berkeley']\n",
    "plot_trends(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pca'] = (\n",
    "   df['title']\n",
    "   .pipe(hero.clean)\n",
    "   .pipe(hero.tfidf)\n",
    "   .pipe(hero.pca)\n",
    ")\n",
    "hero.scatterplot(df, 'pca', color='category', title=\"AI News\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pca'] = (\n",
    "   df['summary']\n",
    "   .pipe(hero.clean)\n",
    "   .pipe(hero.tfidf)\n",
    "   .pipe(hero.pca)\n",
    ")\n",
    "hero.scatterplot(df, 'pca', color='category', title=\"AI News\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pca'] = (\n",
    "   df['text']\n",
    "   .pipe(hero.clean)\n",
    "   .pipe(hero.tfidf)\n",
    "   .pipe(hero.pca)\n",
    ")\n",
    "hero.scatterplot(df, 'pca', color='category', title=\"AI News\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "corpus = st.CorpusFromPandas(df, \n",
    "                              category_col='category', \n",
    "                              text_col='text',\n",
    "                              nlp=nlp).build().compact(st.AssociationCompactor(2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top common words:')\n",
    "pprint(list(corpus.get_scaled_f_scores_vs_background().index[:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cat_explorer(cat):\n",
    "    html = st.produce_scattertext_explorer(corpus,\n",
    "              category=cat,\n",
    "              category_name=cat,\n",
    "              not_category_name='Other',\n",
    "              width_in_pixels=1000,)\n",
    "    open(\"scatterplots/%s Viz.html\"%cat, 'wb').write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in categories:\n",
    "    make_cat_explorer(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_cloud(cat=None):\n",
    "    comment_words = '' \n",
    "    stopwords = set(STOPWORDS) \n",
    "\n",
    "    # iterate through the csv file \n",
    "    for article in all_articles: \n",
    "        if cat is not None and article[0]!=cat:\n",
    "            continue\n",
    "        val = article[-2]\n",
    "        # split the value \n",
    "        tokens = val.split() \n",
    "\n",
    "        # Converts each token into lowercase \n",
    "        for i in range(len(tokens)): \n",
    "            tokens[i] = tokens[i].lower() \n",
    "\n",
    "        comment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                    background_color ='white', \n",
    "                    stopwords = stopwords, \n",
    "                    min_font_size = 10).generate(comment_words) \n",
    "\n",
    "    # plot the WordCloud image                        \n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_word_cloud(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
